{"description": "This complete runnable script trains a CIFAR-10 CNN with two variants: the original network architecture containing all components and an ablation trial where the second convolutional layer (conv2) is removed. In the ablation trial, the conv3 layer is adapted accordingly to accept 32 channels instead of 64, and the subsequent fully-connected layer is updated to match the new feature map size. The code runs both experiments sequentially, logs training and validation metrics per epoch, and collects the results. After training, it prints a summary, creates plots comparing the training and validation accuracy curves of both trials, saves these plots along with a summary to a PDF file, and outputs two JSON files: 'results.json' which contains full experiment metrics and 'ablation_study_results.json' which focuses on validation accuracy vs epochs for all experiments.", "imports": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport json\nimport os\n\n# Ensure directories exist for saving outputs\nos.makedirs('runs', exist_ok=True)\nos.makedirs('results', exist_ok=True)", "executable_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport json\nimport os\n\n# Ensure directories exist for saving outputs\nos.makedirs('runs', exist_ok=True)\nos.makedirs('results', exist_ok=True)\n\n# Define the CNN architecture with parameter for ablation\nclass CIFAR10CNN(nn.Module):\n    def __init__(self, ablate_conv2=False):\n        super(CIFAR10CNN, self).__init__()\n        self.ablate_conv2 = ablate_conv2\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        if not self.ablate_conv2:\n            # Original architecture\n            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  #ABLATABLE_COMPONENT removed in ablation\n            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n            self.fc_input_dim = 128 * 4 * 4  # after three pooling layers\n        else:\n            # Ablation trial: remove conv2 and adjust conv3 accordingly\n            self.conv3 = nn.Conv2d(32, 128, kernel_size=3, padding=1)  \n            # Now only two pooling operations are applied\n            self.fc_input_dim = 128 * 8 * 8\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(self.fc_input_dim, 512)\n        self.fc2 = nn.Linear(512, 10)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # After conv1: 32 x 16 x 16 if pooling applied once\n        if not self.ablate_conv2:\n            x = self.pool(F.relu(self.conv2(x)))  # After conv2: 64 x 8 x 8\n        x = self.pool(F.relu(self.conv3(x)))      # After conv3: \n        # Depending on the architecture, x shape:\n        # Original: 128 x 4 x 4, Ablation: 128 x 8 x 8\n        x = x.view(-1, self.fc_input_dim)\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n# Training function with epoch-wise validation and TensorBoard logging\n# Now returns a dictionary with metrics histories\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=20, run_name='experiment'):\n    # Initialize TensorBoard writer\n    current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n    writer = SummaryWriter(f'runs/{run_name}_{current_time}')\n    \n    best_acc = 0.0\n    history = {\n        'train_loss': [],\n        'train_accuracy': [],\n        'val_loss': [],\n        'val_accuracy': []\n    }\n\n    # Log model graph using a single batch\n    example_images, _ = next(iter(train_loader))\n    writer.add_graph(model, example_images.to(device))\n\n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        epoch_loss = 0.0\n        epoch_correct = 0\n        epoch_total = 0\n\n        for i, data in enumerate(train_loader):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            epoch_total += labels.size(0)\n            epoch_correct += (predicted == labels).sum().item()\n\n            # Print batch progress\n            if i % 100 == 99:\n                print(f'Epoch {epoch + 1}, Batch {i + 1}: training loss: {loss.item():.3f}')\n\n        # Calculate and log training metrics for the epoch\n        avg_train_loss = epoch_loss / len(train_loader)\n        train_accuracy = 100 * epoch_correct / epoch_total\n\n        writer.add_scalar('Training/Loss', avg_train_loss, epoch)\n        writer.add_scalar('Training/Accuracy', train_accuracy, epoch)\n\n        print(f'Epoch {epoch + 1} Training - Avg Loss: {avg_train_loss:.3f}, Accuracy: {train_accuracy:.2f}%')\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for data in val_loader:\n                images, labels = data\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        # Calculate and log validation metrics\n        avg_val_loss = val_loss / len(val_loader)\n        val_accuracy = 100 * val_correct / val_total\n\n        writer.add_scalar('Validation/Loss', avg_val_loss, epoch)\n        writer.add_scalar('Validation/Accuracy', val_accuracy, epoch)\n        # Log learning rate\n        writer.add_scalar('Training/Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n\n        print(f'Epoch {epoch + 1} Validation - Avg Loss: {avg_val_loss:.3f}, Accuracy: {val_accuracy:.2f}%')\n\n        # Log the gap between training and validation metrics\n        writer.add_scalar('Metrics/Train_Val_Loss_Gap', abs(avg_train_loss - avg_val_loss), epoch)\n        writer.add_scalar('Metrics/Train_Val_Accuracy_Gap', abs(train_accuracy - val_accuracy), epoch)\n\n        # Append epoch metrics to history\n        history['train_loss'].append(avg_train_loss)\n        history['train_accuracy'].append(train_accuracy)\n        history['val_loss'].append(avg_val_loss)\n        history['val_accuracy'].append(val_accuracy)\n\n        # Save best model\n        if val_accuracy > best_acc:\n            best_acc = val_accuracy\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_acc': best_acc,\n                'train_loss': avg_train_loss,\n                'val_loss': avg_val_loss,\n            }, f'best_model_{run_name}.pth')\n            print(f'New best model for {run_name} saved with accuracy: {best_acc:.2f}%')\n\n    writer.close()\n    history['best_val_accuracy'] = best_acc\n    return history\n\n# Function to generate summary plots and save PDF and JSON reports\n\ndef generate_reports(results, epochs):\n    # results: dictionary with keys = experiment names, each containing history dictionary with keys:\n    # 'train_loss', 'train_accuracy', 'val_loss', 'val_accuracy', and 'best_val_accuracy'\n\n    pdf_filename = 'results/summary_results.pdf'\n    json_filename = 'results/results.json'\n    ablation_json_filename = 'results/ablation_study_results.json'\n\n    # Create a PDF file to store the plots\n    pp = PdfPages(pdf_filename)\n\n    # Plot training and validation accuracy for each experiment\n    plt.figure(figsize=(10, 6))\n    for exp, hist in results.items():\n        plt.plot(range(1, epochs+1), hist['train_accuracy'], label=f'{exp} Train Accuracy')\n        plt.plot(range(1, epochs+1), hist['val_accuracy'], linestyle='--', label=f'{exp} Val Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Training and Validation Accuracy per Epoch')\n    plt.legend()\n    plt.grid(True)\n    pp.savefig()\n    plt.close()\n\n    # Plot validation accuracy only, comparing experiments\n    plt.figure(figsize=(10, 6))\n    for exp, hist in results.items():\n        plt.plot(range(1, epochs+1), hist['val_accuracy'], label=f'{exp}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Accuracy (%)')\n    plt.title('Validation Accuracy per Epoch (Ablation Study)')\n    plt.legend()\n    plt.grid(True)\n    pp.savefig()\n    plt.close()\n\n    pp.close()\n    print(f'Summary PDF saved as {pdf_filename}')\n\n    # Prepare JSON report with full results\n    with open(json_filename, 'w') as f:\n        json.dump(results, f, indent=4)\n    print(f'Results JSON saved as {json_filename}')\n\n    # Prepare ablation study JSON with only validation accuracy data\n    ablation_results = {}\n    for exp, hist in results.items():\n        ablation_results[exp] = hist['val_accuracy']\n    with open(ablation_json_filename, 'w') as f:\n        json.dump(ablation_results, f, indent=4)\n    print(f'Ablation study JSON saved as {ablation_json_filename}')\n\n    # Also display a summary of best validation accuracies\n    print('\\nSummary of Best Validation Accuracies:')\n    for exp, hist in results.items():\n        print(f'{exp}: {hist[\"best_val_accuracy\"]:.2f}%')\n\n\n# Data loading and training setup in main\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data transforms\n    train_transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomCrop(32, padding=4),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    # Load CIFAR-10 dataset\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                            download=True, transform=train_transform)\n    valset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                          download=True, transform=val_transform)\n\n    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n    valloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=2)\n\n    # Define experiments: original and ablation (remove conv2)\n    experiments = {\n        'original': False,\n        'ablation_no_conv2': True\n    }\n\n    epochs = 20  # You can change the number of epochs\n    overall_results = {}\n\n    # Loop over experiments\n    for exp_name, ablate_flag in experiments.items():\n        print(f'\\nRunning experiment: {exp_name} (ablate_conv2 = {ablate_flag})')\n        # Create appropriate model\n        model = CIFAR10CNN(ablate_conv2=ablate_flag).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n        # Run training\n        history = train_model(model, trainloader, valloader, criterion, optimizer, device, epochs=epochs, run_name=exp_name)\n        overall_results[exp_name] = history\n\n    # After all experiments, generate reports\n    generate_reports(overall_results, epochs)\n\nif __name__ == '__main__':\n    main()"}