This complete runnable script trains a CIFAR-10 CNN with two variants: the original network architecture containing all components and an ablation trial where the second convolutional layer (conv2) is removed. In the ablation trial, the conv3 layer is adapted accordingly to accept 32 channels instead of 64, and the subsequent fully-connected layer is updated to match the new feature map size. The code runs both experiments sequentially, logs training and validation metrics per epoch, and collects the results. After training, it prints a summary, creates plots comparing the training and validation accuracy curves of both trials, saves these plots along with a summary to a PDF file, and outputs two JSON files: 'results.json' which contains full experiment metrics and 'ablation_study_results.json' which focuses on validation accuracy vs epochs for all experiments.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import datetime
from collections import defaultdict
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import json
import os

# Ensure directories exist for saving outputs
os.makedirs('runs', exist_ok=True)
os.makedirs('results', exist_ok=True)

# Define the CNN architecture with parameter for ablation
class CIFAR10CNN(nn.Module):
    def __init__(self, ablate_conv2=False):
        super(CIFAR10CNN, self).__init__()
        self.ablate_conv2 = ablate_conv2
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        if not self.ablate_conv2:
            # Original architecture
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  #ABLATABLE_COMPONENT removed in ablation
            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
            self.fc_input_dim = 128 * 4 * 4  # after three pooling layers
        else:
            # Ablation trial: remove conv2 and adjust conv3 accordingly
            self.conv3 = nn.Conv2d(32, 128, kernel_size=3, padding=1)  
            # Now only two pooling operations are applied
            self.fc_input_dim = 128 * 8 * 8
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(self.fc_input_dim, 512)
        self.fc2 = nn.Linear(512, 10)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # After conv1: 32 x 16 x 16 if pooling applied once
        if not self.ablate_conv2:
            x = self.pool(F.relu(self.conv2(x)))  # After conv2: 64 x 8 x 8
        x = self.pool(F.relu(self.conv3(x)))      # After conv3: 
        # Depending on the architecture, x shape:
        # Original: 128 x 4 x 4, Ablation: 128 x 8 x 8
        x = x.view(-1, self.fc_input_dim)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# Training function with epoch-wise validation and TensorBoard logging
# Now returns a dictionary with metrics histories

def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=20, run_name='experiment'):
    # Initialize TensorBoard writer
    current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
    writer = SummaryWriter(f'runs/{run_name}_{current_time}')
    
    best_acc = 0.0
    history = {
        'train_loss': [],
        'train_accuracy': [],
        'val_loss': [],
        'val_accuracy': []
    }

    # Log model graph using a single batch
    example_images, _ = next(iter(train_loader))
    writer.add_graph(model, example_images.to(device))

    for epoch in range(epochs):
        # Training phase
        model.train()
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0

        for i, data in enumerate(train_loader):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            epoch_total += labels.size(0)
            epoch_correct += (predicted == labels).sum().item()

            # Print batch progress
            if i % 100 == 99:
                print(f'Epoch {epoch + 1}, Batch {i + 1}: training loss: {loss.item():.3f}')

        # Calculate and log training metrics for the epoch
        avg_train_loss = epoch_loss / len(train_loader)
        train_accuracy = 100 * epoch_correct / epoch_total

        writer.add_scalar('Training/Loss', avg_train_loss, epoch)
        writer.add_scalar('Training/Accuracy', train_accuracy, epoch)

        print(f'Epoch {epoch + 1} Training - Avg Loss: {avg_train_loss:.3f}, Accuracy: {train_accuracy:.2f}%')

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for data in val_loader:
                images, labels = data
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        # Calculate and log validation metrics
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * val_correct / val_total

        writer.add_scalar('Validation/Loss', avg_val_loss, epoch)
        writer.add_scalar('Validation/Accuracy', val_accuracy, epoch)
        # Log learning rate
        writer.add_scalar('Training/Learning_Rate', optimizer.param_groups[0]['lr'], epoch)

        print(f'Epoch {epoch + 1} Validation - Avg Loss: {avg_val_loss:.3f}, Accuracy: {val_accuracy:.2f}%')

        # Log the gap between training and validation metrics
        writer.add_scalar('Metrics/Train_Val_Loss_Gap', abs(avg_train_loss - avg_val_loss), epoch)
        writer.add_scalar('Metrics/Train_Val_Accuracy_Gap', abs(train_accuracy - val_accuracy), epoch)

        # Append epoch metrics to history
        history['train_loss'].append(avg_train_loss)
        history['train_accuracy'].append(train_accuracy)
        history['val_loss'].append(avg_val_loss)
        history['val_accuracy'].append(val_accuracy)

        # Save best model
        if val_accuracy > best_acc:
            best_acc = val_accuracy
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
            }, f'best_model_{run_name}.pth')
            print(f'New best model for {run_name} saved with accuracy: {best_acc:.2f}%')

    writer.close()
    history['best_val_accuracy'] = best_acc
    return history

# Function to generate summary plots and save PDF and JSON reports

def generate_reports(results, epochs):
    # results: dictionary with keys = experiment names, each containing history dictionary with keys:
    # 'train_loss', 'train_accuracy', 'val_loss', 'val_accuracy', and 'best_val_accuracy'

    pdf_filename = 'results/summary_results.pdf'
    json_filename = 'results/results.json'
    ablation_json_filename = 'results/ablation_study_results.json'

    # Create a PDF file to store the plots
    pp = PdfPages(pdf_filename)

    # Plot training and validation accuracy for each experiment
    plt.figure(figsize=(10, 6))
    for exp, hist in results.items():
        plt.plot(range(1, epochs+1), hist['train_accuracy'], label=f'{exp} Train Accuracy')
        plt.plot(range(1, epochs+1), hist['val_accuracy'], linestyle='--', label=f'{exp} Val Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title('Training and Validation Accuracy per Epoch')
    plt.legend()
    plt.grid(True)
    pp.savefig()
    plt.close()

    # Plot validation accuracy only, comparing experiments
    plt.figure(figsize=(10, 6))
    for exp, hist in results.items():
        plt.plot(range(1, epochs+1), hist['val_accuracy'], label=f'{exp}')
    plt.xlabel('Epoch')
    plt.ylabel('Validation Accuracy (%)')
    plt.title('Validation Accuracy per Epoch (Ablation Study)')
    plt.legend()
    plt.grid(True)
    pp.savefig()
    plt.close()

    pp.close()
    print(f'Summary PDF saved as {pdf_filename}')

    # Prepare JSON report with full results
    with open(json_filename, 'w') as f:
        json.dump(results, f, indent=4)
    print(f'Results JSON saved as {json_filename}')

    # Prepare ablation study JSON with only validation accuracy data
    ablation_results = {}
    for exp, hist in results.items():
        ablation_results[exp] = hist['val_accuracy']
    with open(ablation_json_filename, 'w') as f:
        json.dump(ablation_results, f, indent=4)
    print(f'Ablation study JSON saved as {ablation_json_filename}')

    # Also display a summary of best validation accuracies
    print('\nSummary of Best Validation Accuracies:')
    for exp, hist in results.items():
        print(f'{exp}: {hist["best_val_accuracy"]:.2f}%')


# Data loading and training setup in main

def main():
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Data transforms
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomCrop(32, padding=4),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    # Load CIFAR-10 dataset
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=train_transform)
    valset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                          download=True, transform=val_transform)

    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
    valloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=2)

    # Define experiments: original and ablation (remove conv2)
    experiments = {
        'original': False,
        'ablation_no_conv2': True
    }

    epochs = 20  # You can change the number of epochs
    overall_results = {}

    # Loop over experiments
    for exp_name, ablate_flag in experiments.items():
        print(f'\nRunning experiment: {exp_name} (ablate_conv2 = {ablate_flag})')
        # Create appropriate model
        model = CIFAR10CNN(ablate_conv2=ablate_flag).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

        # Run training
        history = train_model(model, trainloader, valloader, criterion, optimizer, device, epochs=epochs, run_name=exp_name)
        overall_results[exp_name] = history

    # After all experiments, generate reports
    generate_reports(overall_results, epochs)

if __name__ == '__main__':
    main()